{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c28a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbd9402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_num = 10\n",
    "feature_num = 24\n",
    "word_num = 5\n",
    "batch_size = 15\n",
    "dic_num = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4e544b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(word_num, feature_num, dtype = torch.float32)\n",
    "k = torch.rand(word_num, feature_num, dtype = torch.float32)\n",
    "v = torch.rand(word_num, feature_num, dtype = torch.float32)\n",
    "word = torch.randint(0,20,size=(batch_size,seq_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "985cbea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, word_num, feature_num):\n",
    "        super().__init__()\n",
    "        self.w = word_num\n",
    "        self.d = feature_num\n",
    "        self.embedding = nn.Embedding(word_num, feature_num)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.embedding(X) * math.sqrt(feature_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1962660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_emd = InputEmbedding(dic_num, feature_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d003293",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedd = input_emd(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "679e2a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, word_num,feature_num):\n",
    "        super().__init__()\n",
    "        self.zeros = torch.zeros(word_num, feature_num)\n",
    "        sfeature_num = torch.arange(0,feature_num,dtype=torch.float)        \n",
    "        for i in range(word_num):\n",
    "            self.zeros[i, 0::2] = torch.sin(i / torch.pow(10000, 2 * sfeature_num / math.sqrt(feature_num)))[0::2]\n",
    "            self.zeros[i, 1::2] = torch.cos(i / torch.pow(10000, 2 * sfeature_num / math.sqrt(feature_num)))[1::2]\n",
    "    def forward(self, x):\n",
    "        return self.zeros + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc0c1641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class revised_PositionEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, word_num,feature_num, p=0.2):\n",
    "        super().__init__()\n",
    "        even_i = torch.pow(10000, 2 * torch.arange(0, feature_num, 2, dtype=torch.float)/math.sqrt(feature_num))\n",
    "        odd_i = torch.pow(10000, 2 * torch.arange(1, feature_num, 2, dtype=torch.float)/math.sqrt(feature_num))\n",
    "        tword = torch.arange(0, word_num).unsqueeze(1)\n",
    "        self.pe = torch.zeros(word_num, feature_num)\n",
    "        self.pe[:, 0::2] = torch.sin(tword / even_i)\n",
    "        self.pe[:, 1::2] = torch.cos(tword / odd_i)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p)\n",
    "    def forward(self, x):\n",
    "        x = (self.pe + x).detach()\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75ff94ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_emd = PositionEmbedding(word_num = seq_num,feature_num = feature_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a031d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_emb = position_emd(embedd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3327bbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_pos_emd = revised_PositionEmbedding(word_num = seq_num,feature_num = feature_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c147456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_total_emd = revised_pos_emd(embedd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f27c340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    \n",
    "    def __init__(self, eps = 10 ** -6):\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.rand(1))\n",
    "        self.beta = nn.Parameter(torch.rand(1))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.alpha * (x-mean) / (std**2 + self.eps)**0.5 + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cad1d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_norm = LayerNormalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04ca2e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_embed = layer_norm(revised_total_emd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "696513f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, p =0.2):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(input_dim, 48),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(48,output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feea912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = FCLayer(feature_num, feature_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a527df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_embed = fc(normed_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bb54363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 10, 24])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd248652",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9377ed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "twq = torch.rand(feature_num, feature_num, dtype = torch.float32)\n",
    "twk = torch.rand(feature_num, feature_num, dtype = torch.float32)\n",
    "twv = torch.rand(feature_num, feature_num, dtype = torch.float32)        \n",
    "thead_num = h\n",
    "tfeature_num = feature_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d978cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_num, feature_num, p=0.2):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Linear(feature_num, feature_num)\n",
    "        self.wk = nn.Linear(feature_num, feature_num)\n",
    "        self.wv = nn.Linear(feature_num, feature_num)\n",
    "        self.do = nn.Dropout(p=p)\n",
    "        self.head_num = head_num\n",
    "        self.feature_num = feature_num\n",
    "        self.dk = feature_num // self.head_num\n",
    "    \n",
    "    @staticmethod\n",
    "    def attention(q, k, v, dk ,mask):\n",
    "        qk = torch.matmul(q, k.transpose(-1,-2)) / math.sqrt(dk)\n",
    "        if mask is not None:\n",
    "            qk.masked_fill_(mask==0, -1e9)\n",
    "        attention = qk.softmax(dim=-1)\n",
    "        attention = torch.matmul(attention, v)\n",
    "        return attention\n",
    "    \n",
    "    def forward(self,q,k,v, mask):\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)        \n",
    "        v = self.wv(v)\n",
    "        q = q.view(q.shape[0], q.shape[1], self.head_num, self.dk).transpose(1,2)\n",
    "        k = k.view(k.shape[0], k.shape[1], self.head_num, self.dk).transpose(1,2) \n",
    "        v = v.view(v.shape[0], v.shape[1], self.head_num, self.dk).transpose(1,2)\n",
    "        attention = MultiHead.attention(q,k,v, self.dk, mask)\n",
    "        attention = attention.transpose(1,2).contiguous().view(attention.shape[0], -1, self.feature_num)\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb5cbd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "    def __init__(self, p=0.2):\n",
    "        super().__init__()\n",
    "        self.dp = nn.Dropout(p=p)\n",
    "        self.norm = LayerNormalization()\n",
    "    \n",
    "    def forward(self,x,sublayer):\n",
    "        return x + self.dp(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57d74ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, attention_layer, ff_layer):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.attention_layer = attention_layer\n",
    "        self.ff_layer = ff_layer\n",
    "        self.block = nn.ModuleList([ResidualConnection() for i in range(2)])\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        x = self.block[0](x, lambda x: self.attention_layer(x,x,x,mask))\n",
    "        x = self.block[1](x, self.ff_layer)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8203b2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, n=6):\n",
    "        self.layer = layer\n",
    "        self.n = n\n",
    "        self.norm = LayerNormalization()\n",
    "    def forward(self, x, mask):\n",
    "        for b in self.layer:\n",
    "            x = b(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59b6713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, self_attention_layer, cross_attention_layer, ff_layer):\n",
    "        super().__init__()\n",
    "        self.self_attention_layer = self_attention_layer\n",
    "        self.cross_attention_layer = cross_attention_layer\n",
    "        self.ff_layer = ff_layer\n",
    "        self.block = nn.ModuleList([ResidualConnection() for i in range(3)])\n",
    "        \n",
    "    def forward(self, encoder_output, src_mask, tgt_mask):\n",
    "        \n",
    "        x = self.block[0](x, lambda x: self.self_attention_layer(x,x,x, src_mask))\n",
    "        x = self.block[1](x, lambda x: self.cross_attention_layer(x,encoder_output,encoder_output, src_mask))\n",
    "        x = self.block[2](x, self.ff_layer)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2187a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, layer):\n",
    "        self.layer = layer\n",
    "        self.norm = LayerNormalization()\n",
    "    \n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for l in self.layer:\n",
    "            x = l(x, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "484f9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, feature_num, dic_num):\n",
    "        \n",
    "        self.linear = nn.Linear(feature_num, dic_num)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.linear(x)\n",
    "        return torch.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a5f9959",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pf/h34mys2s64v2ntfrmcjcmtq00000gn/T/ipykernel_5692/2007274368.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_pos_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_pos_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_emb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_emb, tgt_emb, src_pos_emb, tgt_pos_emb, encoder, decoder, projection_layer):\n",
    "        super().__init__()\n",
    "        self.src_emb = src_emb\n",
    "        self.tgt_emb = tgt_emb\n",
    "        self.src_pos_emb = src_pos_emb\n",
    "        self.tgt_pos_emb = tgt_pos_emb\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.projection = projection_layer\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_emb(src)\n",
    "        src = self.src_pos_emb(src)\n",
    "        return self.encoder(x, src_mask)\n",
    "    \n",
    "    def decoder(self, tgt, src, src_mask, tgt_mask):\n",
    "        tgt = self.tgt_emb(tgt)\n",
    "        tgt = self.tgt_pos_emb(tgt)\n",
    "        return self.decoder(tgt, src, src_mask, tgt_mask)\n",
    "    \n",
    "    def projection(self,x):\n",
    "        return self.projection(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19c6853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initTransformer(src_voc_num, tgt_voc_num, d_model=24, n=6, head_num=3):\n",
    "    src_emb = InputEmbedding(src_voc_num, d_model)\n",
    "    tgt_emb = InputEmbedding(tgt_voc_num, d_model)\n",
    "    src_pos_emb = revised_PositionEmbedding(src_voc_num, d_model)\n",
    "    tgt_pos_emb = revised_PositionEmbedding(pos_voc_num, d_model)\n",
    "    projection = ProjectionLayer(d_model, tgt_voc_num)\n",
    "    encoder = []\n",
    "    for i in range(n):\n",
    "        mh = MultiHead(head_num, d_model)\n",
    "        fc = FClayer(d_model, d_model)\n",
    "        encoderblock = EncoderBlock(mh, fc)\n",
    "        encoder.append(encoderblock)\n",
    "    encoder = Encoder(nn.ModuleList(encoder))\n",
    "    \n",
    "    decoder = []\n",
    "    for i in range(n):\n",
    "        self_attn = MultiHead(head_num, d_model)\n",
    "        cross_attn = MultiHead(head_num, d_model)\n",
    "        fc = FClayer(d_model, d_model)\n",
    "        decoderblock = DecoderBlock(self_attn, cross_attn, fc)\n",
    "        decoder.append(encoderblock)\n",
    "    \n",
    "    decoder = Decoder(nn.ModuleList(decoder))\n",
    "    \n",
    "    transformer = Transformer(src_emb, tgt_emb, src_pos_emb, tgt_pos_emb, encoder, decoder, projection)\n",
    "    \n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058cfbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
